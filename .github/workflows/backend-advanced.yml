name: ðŸ Backend Advanced Tests

on:
  push:
    paths:
      - 'backend/**'
      - 'ghl_integration/**'
      - 'requirements.txt'
      - 'manage.py'
      - '*.py'
  pull_request:
    paths:
      - 'backend/**'
      - 'ghl_integration/**'
      - 'requirements.txt'
      - 'manage.py'
      - '*.py'
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM
  workflow_dispatch:

jobs:
  # ðŸ§ª Backend Unit Tests with Multiple Python & DB Versions
  backend-matrix-tests:
    name: ðŸ§ª Backend Tests Matrix
    runs-on: ${{ matrix.os }}
    
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest]
        python-version: ['3.8', '3.9', '3.10', '3.11']
        database: [sqlite, postgres]
        exclude:
          - os: windows-latest
            database: postgres
    
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_ghl_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4
      
    - name: ðŸ Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: ðŸ“¦ Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cache/pip
          ~/.cache/pipenv
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-
          ${{ runner.os }}-pip-
          
    - name: ðŸ“‹ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-django pytest-cov pytest-xdist pytest-mock
        pip install bandit safety
        
    - name: ðŸ”§ Set up test environment
      run: |
        cp .env.example .env
        echo "DEBUG=True" >> .env
        echo "GHL_MOCK=True" >> .env
        echo "SECRET_KEY=test-secret-key-for-ci" >> .env
        
    - name: ðŸ”§ Configure database
      run: |
        if [ "${{ matrix.database }}" = "postgres" ]; then
          echo "DATABASE_URL=postgres://postgres:postgres@localhost:5432/test_ghl_db" >> .env
        else
          echo "DATABASE_URL=sqlite:///test_db.sqlite3" >> .env
        fi
      shell: bash
        
    - name: ðŸ—„ï¸ Run migrations
      run: |
        python manage.py migrate --verbosity=2
        
    - name: ðŸ§ª Run comprehensive tests
      run: |
        pytest -v \
          --cov=ghl_integration \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term-missing \
          --junit-xml=junit.xml \
          --tb=short \
          -n auto
          
    - name: ðŸ“Š Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: backend-${{ matrix.os }}-py${{ matrix.python-version }}-${{ matrix.database }}
        name: backend-coverage-${{ matrix.os }}-py${{ matrix.python-version }}-${{ matrix.database }}
        fail_ci_if_error: false
        
    - name: ðŸ“ˆ Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.os }}-py${{ matrix.python-version }}-${{ matrix.database }}
        path: |
          junit.xml
          htmlcov/
          coverage.xml

  # ðŸ”’ Security Scanning
  security-scan:
    name: ðŸ”’ Security Scan
    runs-on: ubuntu-latest
    
    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4
      
    - name: ðŸ Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: ðŸ“¦ Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit[toml] safety semgrep
        pip install -r requirements.txt
        
    - name: ðŸ” Bandit security scan
      run: |
        bandit -r . -f json -o bandit-report.json
        bandit -r . -f txt
      continue-on-error: true
      
    - name: ðŸ“¦ Dependency vulnerability scan
      run: |
        safety check --json --output safety-report.json
        safety check
      continue-on-error: true
      
    - name: ðŸ” Semgrep security scan
      run: |
        semgrep --config=auto --json --output=semgrep-report.json .
        semgrep --config=auto .
      continue-on-error: true
      
    - name: ðŸ“ˆ Upload security reports
      uses: actions/upload-artifact@v3
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json
          semgrep-report.json

  # âš¡ Performance Testing
  performance-test:
    name: âš¡ Performance Tests
    runs-on: ubuntu-latest
    
    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4
      
    - name: ðŸ Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: ðŸ“¦ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install django-silk locust pytest-benchmark
        
    - name: ðŸ”§ Set up test environment
      run: |
        cp .env.example .env
        echo "DEBUG=False" >> .env
        echo "GHL_MOCK=True" >> .env
        echo "SECRET_KEY=test-secret-key-for-performance" >> .env
        
    - name: ðŸ—„ï¸ Setup database
      run: |
        python manage.py migrate
        python manage.py collectstatic --noinput
        
    - name: âš¡ Run performance benchmarks
      run: |
        # Create a simple performance test
        cat > performance_test.py << 'EOF'
        import requests
        import time
        import json
        from concurrent.futures import ThreadPoolExecutor
        import subprocess
        import threading
        
        def start_server():
            subprocess.run(['python', 'manage.py', 'runserver', '8000'], 
                         stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        
        def test_endpoint(url):
            try:
                start_time = time.time()
                response = requests.get(url, timeout=10)
                end_time = time.time()
                return {
                    'url': url,
                    'status': response.status_code,
                    'time': end_time - start_time,
                    'success': response.status_code == 200
                }
            except Exception as e:
                return {
                    'url': url,
                    'status': 'error',
                    'time': 0,
                    'success': False,
                    'error': str(e)
                }
        
        # Start Django server in background
        server_thread = threading.Thread(target=start_server)
        server_thread.daemon = True
        server_thread.start()
        
        # Wait for server to start
        time.sleep(10)
        
        # Test endpoints
        endpoints = [
            'http://localhost:8000/api/ghl/debug/',
            'http://localhost:8000/api/ghl/ping/',
            'http://localhost:8000/api/ghl/calendars/',
            'http://localhost:8000/api/ghl/rate-limit/',
        ]
        
        print("ðŸš€ Running performance tests...")
        
        # Sequential tests
        print("\nðŸ“Š Sequential Tests:")
        sequential_results = []
        for endpoint in endpoints:
            result = test_endpoint(endpoint)
            sequential_results.append(result)
            print(f"  {endpoint}: {result['time']:.3f}s - {'âœ…' if result['success'] else 'âŒ'}")
        
        # Concurrent tests
        print("\nðŸ“Š Concurrent Tests (10 requests per endpoint):")
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = []
            for endpoint in endpoints:
                for _ in range(10):
                    futures.append(executor.submit(test_endpoint, endpoint))
            
            concurrent_results = []
            for future in futures:
                concurrent_results.append(future.result())
        
        # Calculate stats
        successful_requests = [r for r in concurrent_results if r['success']]
        if successful_requests:
            avg_time = sum(r['time'] for r in successful_requests) / len(successful_requests)
            max_time = max(r['time'] for r in successful_requests)
            min_time = min(r['time'] for r in successful_requests)
            success_rate = len(successful_requests) / len(concurrent_results) * 100
            
            print(f"\nðŸ“ˆ Performance Summary:")
            print(f"  Success Rate: {success_rate:.1f}%")
            print(f"  Average Response Time: {avg_time:.3f}s")
            print(f"  Min Response Time: {min_time:.3f}s")
            print(f"  Max Response Time: {max_time:.3f}s")
            print(f"  Total Requests: {len(concurrent_results)}")
            
            # Save results
            with open('performance_results.json', 'w') as f:
                json.dump({
                    'sequential': sequential_results,
                    'concurrent': concurrent_results,
                    'summary': {
                        'success_rate': success_rate,
                        'avg_time': avg_time,
                        'min_time': min_time,
                        'max_time': max_time,
                        'total_requests': len(concurrent_results)
                    }
                }, f, indent=2)
        else:
            print("âŒ No successful requests")
        EOF
        
        python performance_test.py
        
    - name: ðŸ“ˆ Upload performance results
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: performance_results.json

  # ðŸ“Š Code Quality Analysis  
  code-quality:
    name: ðŸ“Š Code Quality Analysis
    runs-on: ubuntu-latest
    
    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4
      
    - name: ðŸ Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: ðŸ“¦ Install quality tools
      run: |
        python -m pip install --upgrade pip
        pip install radon xenon mccabe vulture mypy pylint
        pip install -r requirements.txt
        
    - name: ðŸ“Š Complexity analysis
      run: |
        echo "## ðŸ“Š Code Complexity Analysis" >> $GITHUB_STEP_SUMMARY
        
        # Cyclomatic complexity
        echo "### Cyclomatic Complexity:" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        radon cc . --total-average >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        
        # Maintainability index
        echo "### Maintainability Index:" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        radon mi . >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        
        # Raw metrics
        radon raw . --json > raw_metrics.json
        radon cc . --json > complexity_metrics.json
        radon mi . --json > maintainability_metrics.json
        
    - name: ðŸ” Dead code detection
      run: |
        vulture . --json > vulture_report.json || true
        echo "### ðŸ” Dead Code Detection:" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        vulture . >> $GITHUB_STEP_SUMMARY || true
        echo '```' >> $GITHUB_STEP_SUMMARY
        
    - name: ðŸ“ˆ Upload quality reports
      uses: actions/upload-artifact@v3
      with:
        name: code-quality-reports
        path: |
          raw_metrics.json
          complexity_metrics.json
          maintainability_metrics.json
          vulture_report.json
